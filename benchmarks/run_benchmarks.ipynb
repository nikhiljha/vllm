{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "import aiohttp\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'meta-llama/Llama-2-7b-hf'\n",
    "dataset = 'ShareGPT_V3_unfiltered_cleaned_split.json'\n",
    "block_size = 32\n",
    "\n",
    "prefix_len = 32\n",
    "\n",
    "# feel free to tweak these values. they currently assume:\n",
    "# - 16 GPU prefixes\n",
    "# - 32 CPU prefixes\n",
    "# - 64 Disk prefixes\n",
    "prefixes_gpu = 8\n",
    "prefixes_gpu_cpu = 32\n",
    "prefixes_all = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "# create file and write csv header\n",
    "filename = f\"results/{int(time.time())}.csv\"    \n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(\"benchmark,latency (s),throughput (req/s),hits_gpu,hits_cpu,hits_disk,misses,swaps_gpu_cpu,swaps_gpu_disk,swaps_cpu_gpu,swaps_cpu_disk,swaps_disk_gpu,swaps_disk_cpu,util_gpu,util_cpu,util_disk\\n\")\n",
    "    \n",
    "print(f\"Saving results to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency_cmd = \"\"\"python benchmark_latency.py \\\n",
    "    --model \\\"{}\\\" \\\n",
    "    --batch-size 256 \\\n",
    "    --input-len 64 \\\n",
    "    --num-prefixes {} \\\n",
    "    --prefix-len {} \\\n",
    "    --output-csv \\\"{}\\\" \\\n",
    "    --benchmark-name \\\"{}\\\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(latency_cmd.format(model, 1, 0, filename, 'latency_baseline'), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(latency_cmd.format(model, prefixes_gpu, prefix_len, filename, 'latency_gpu'), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(latency_cmd.format(model, prefixes_gpu_cpu, prefix_len, filename, 'latency_gpu_cpu'), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(latency_cmd.format(model, prefixes_all, prefix_len, filename, 'latency_all'), shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "throughput_cmd = \"\"\"python benchmark_throughput.py \\\n",
    "    --model \\\"{}\\\" \\\n",
    "    --input-len 512 \\\n",
    "    --output-len 128 \\\n",
    "    --num-prompts 256 \\\n",
    "    --num-prefixes {} \\\n",
    "    --prefix-len {} \\\n",
    "    --output-csv \\\"{}\\\" \\\n",
    "    --benchmark-name \\\"{}\\\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(throughput_cmd.format(model, 1, 0, filename, 'throughput_baseline'), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(throughput_cmd.format(model, prefixes_gpu, prefix_len, filename, 'throughput_gpu'), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(throughput_cmd.format(model, prefixes_gpu_cpu, prefix_len, filename, 'throughput_gpu_cpu'), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(throughput_cmd.format(model, prefixes_all, prefix_len, filename, 'throughput_all'), shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_proc = subprocess.Popen(f\"python -m vllm.entrypoints.api_server --model \\\"{model}\\\"\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"waiting for API server to start...\")\n",
    "while True:\n",
    "    try:\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(f\"http://0.0.0.0:8000/health\") as response:\n",
    "                if response.status == 200:\n",
    "                    break\n",
    "    except:\n",
    "        pass\n",
    "    await asyncio.sleep(1)\n",
    "print(\"server started!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_cmd = \"\"\"python benchmark_serving.py \\\n",
    "    --tokenizer \\\"{}\\\" \\\n",
    "    --dataset \\\"{}\\\" \\\n",
    "    --num-prompts 256 \\\n",
    "    --system-prompt \\\"system_prompt.txt\\\" \\\n",
    "    --block-size {} \\\n",
    "    --use-prefix {} \\\n",
    "    --output-csv \\\"{}\\\" \\\n",
    "    --benchmark-name \\\"{}\\\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(serving_cmd.format(model, dataset, block_size, 'false', filename, 'serving_baseline'), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(serving_cmd.format(model, dataset, block_size, 'true', filename, 'serving_prefix'), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stopping API server\")\n",
    "subprocess.run(f\"kill -9 {server_proc.pid + 1}\", shell=True) # not sure why PID is one less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm-c12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
